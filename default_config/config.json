{
    "llama-2-7b-chat.Q4_K_M": {
        "prompt": "<|im_start|> system\n{system_prompt}\n<|im_end|>\n<|im_start|> user\n{user_prompt}\n<|im_end|>\n<|im_start|> assistant\n",
        "loader_config": {
            "n_ctx": 4096,
            "max_tokens": 2048,
            "stop": ["<|im_end|>"]
        }
    },
    "gpt4all-falcon-q4_0": {
        "prompt": "### Instruction: {system_prompt}\n{user_prompt}\n### Response:",
        "loader_config": {
            "n_ctx": 4096,
            "max_tokens": 2048,
            "stop": ["### Instruction:"]
        }
    },
    "leo-hessianai-13b-chat-bilingual.Q4_K_M": {
        "prompt": "<|im_start|> system\n{system_prompt}\n<|im_end|>\n<|im_start|> user\n{user_prompt}\n<|im_end|>\n<|im_start|> assistant\n",
        "loader_config": {
            "n_ctx": 4096,
            "max_tokens": 2048,
            "stop": ["<|im_end|>"]
        }
    },
    "neuralbeagle14-7b.Q4_K_M": {
        "prompt": "<|im_start|> system\n{system_prompt}\n<|im_end|>\n<|im_start|> user\n{user_prompt}\n<|im_end|>\n<|im_start|> assistant\n",
        "loader_config": {
            "n_ctx": 8000,
            "max_tokens": 4000,
            "stop": ["<|im_end|>"]
        }
    },
    "Meta-Llama-3-8B-Instruct.Q4_K_M": {
        "prompt": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n{user_prompt}<|eot_id|>\n<|start_header_id|>assistant<|end_header_id|>\n",
        "loader_config": {
            "n_ctx": 8000,
            "max_tokens": 4000,
            "stop": ["<|eot_id|>"],
            "temperature": 0.3
        }
    },
    "Meta-Llama-3.1-8B-Instruct-Q4_K_M": {
        "prompt": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n{user_prompt}<|eot_id|>\n<|start_header_id|>assistant<|end_header_id|>\n",
        "loader_config": {
            "n_ctx": 16384,
            "max_tokens": 8196,
            "stop": ["<|eot_id|>"],
            "temperature": 0.3
        }
    },
    "Qwen2.5-7B-Instruct.Q4_K_M": {
        "prompt": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n{user_prompt}<|eot_id|>\n<|start_header_id|>assistant<|end_header_id|>\n",
        "loader_config": {
            "n_ctx": 8224,
            "max_tokens": 8196,
            "stop": ["<|eot_id|>"],
            "temperature": 0.15
        }
    },
    "olmo-2-1124-7B-instruct-Q4_K_M": {
        "prompt": "<|endoftext|><|system|>\n{system_prompt}\n<|user|>\n{system_prompt}\n{user_prompt}\n<|assistant|>\n",
        "loader_config": {
            "n_ctx": 4096,
            "max_tokens": 2048,
            "stop": ["<|endoftext|>"],
            "temperature": 0.7
        }
    },
    "olmo-2-1124-13B-instruct-Q5_K_M": {
        "prompt": "<|endoftext|><|system|>\n{system_prompt}\n<|user|>\n{system_prompt}\n{user_prompt}\n<|assistant|>\n",
        "loader_config": {
            "n_ctx": 4096,
            "max_tokens": 2048,
            "stop": ["<|endoftext|>"],
            "temperature": 0.7
        }
    },
    "Mistral-Small-24B-Instruct-2501-Q3_K_M": {
        "prompt": "<|endoftext|><|system|>\n{system_prompt}\n<|user|>\n{system_prompt}\n{user_prompt}\n<|assistant|>\n",
        "loader_config": {
            "n_ctx": 32768,
            "max_tokens": 2048,
            "stop": ["<|endoftext|>"],
            "temperature": 0.3
        }
    },
    "google_gemma-3-12b-it-qat-Q4_0": {
        "loader_config": {
            "n_ctx": 8192,
            "max_tokens": 2048,
            "temperature": 0.3
        }
    },
    "default": {
        "prompt": "<|im_start|> system\n{system_prompt}\n<|im_end|>\n<|im_start|> user\n{user_prompt}\n<|im_end|>\n<|im_start|> assistant\n",
        "loader_config": {
            "n_ctx": 4096,
            "max_tokens": 2048,
            "stop": ["<|im_end|>"],
            "temperature": 0.6
        }
    }
}
